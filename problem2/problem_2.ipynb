{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1\n",
    "In this notebook, we use the following modules `numpy` and `minotaur_maze`. The latter is a home made module, where all the solutions to the questions are implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import minoutaur_maze as mz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: The Maze and the Random Minotaur\n",
    "\n",
    "The objective of problem 1 is to solve the shortest path problem in a maze. We start first by describing the maze as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting positions\n",
    "thomas_st = (0,0)\n",
    "minotaur_st = (6,5)\n",
    "\n",
    "# no need to pick key (we have it from the beginning)\n",
    "key_st = (1,)\n",
    "\n",
    "# starting state\n",
    "start = (thomas_st + minotaur_st + key_st)\n",
    "\n",
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "# with the convention \n",
    "# 0 = empty cell\n",
    "# 1 = obstacle\n",
    "# 2 = exit of the Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `maze.draw_maze()` helps us draw the maze given its numpy array discription.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE MINOTAUR CANNOT WAIT   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "minotaur_can_wait = False\n",
    "key_needed = False\n",
    "env_1 = mz.MinotaurMaze(maze,minotaur_can_wait,key_needed)\n",
    "#env_1.show()\n",
    "print('# states = ', env_1.n_states)\n",
    "print('# actions = ', env_1.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dynamic Programming \n",
    "\n",
    "Before solving the MDP problem, recall that the finite horizon objective function is \n",
    "$$\n",
    "    \\mathbb{E} \\Big[ \\sum_{t=0}^T r(s_t, a_t) \\Big],\n",
    "$$\n",
    "where $T$ is the horizon.\n",
    "Recall the Bellman equation \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V(s) = \\max_{a \\in \\mathcal{A}} \\Big\\lbrace r(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,a) V(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "The dynamic programming solution for the finite horizon MDP problem consists of solving the above backward recursion. The method `maze.dynamic_programming` achieves this. \n",
    "> **Note:** To find the optimal path, it is enough to set the time horizon $T = 10$. Indeed, looking at the maze one can see that the player needs at least 10 steps to attain the exit $B$, if her starting position is at $A$. In fact if you set the time horizon less than 10, you will see that you do not find the optimal path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 17\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy = mz.dynamic_programming(env_1,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'DynProg'\n",
    "path, victory_prob = env_1.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 1\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_probs = env_1.minotaur_cell_probs(maze,minotaur_st,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = horizon\n",
    "plt.imshow(cell_probs[t,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victory prob if the path is fixed\n",
    "fixed_path_vict_prob = env_1.fixed_path_vict_prob(maze,path,cell_probs)\n",
    "fixed_path_vict_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY THAT MAXIMIZES PROB. OF LEAVING FOR T = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy = mz.dynamic_programming(env_1,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several paths and compute victory prob\n",
    "method = 'DynProg'\n",
    "N = 5000\n",
    "paths_dict = {}\n",
    "probs_dict = {}\n",
    "probs = []\n",
    "for n in range(N) :\n",
    "    path, victory_prob = env_1.simulate(start, policy, method)\n",
    "    paths_dict[n] = path\n",
    "    probs_dict[n] = victory_prob\n",
    "    probs.append(victory_prob)\n",
    "\n",
    "probs.sort()\n",
    "print('# SIMULATIONS = ', N)\n",
    "print('Mean Victory Prob = ', sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY THAT MAXIMIZES PROB FOR T = {1, ..., 30} AND PROBABILITY OF LEAVING THE MAZE FOR EACH T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_min = 1\n",
    "T_max = 30\n",
    "horizons = range(T_min,T_max+1)\n",
    "Vs, policys = [], []\n",
    "for horizon in horizons :\n",
    "    V, policy = mz.dynamic_programming(env_1,horizon)\n",
    "    Vs.append(V)\n",
    "    policys.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'DynProg'\n",
    "N = 5000\n",
    "# Simulate N paths for each time horizon\n",
    "mean_victory_probs = []\n",
    "for i in range(T_max) :\n",
    "    probs = []\n",
    "    for n in range(N) :\n",
    "        path, victory_prob = env_1.simulate(start, policys[i], method)\n",
    "        probs.append(victory_prob)\n",
    "    # Compute mean victory prob\n",
    "    mean_victory_probs.append(sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean victory prob for each horizon\n",
    "plt.plot(horizons, mean_victory_probs, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE MINOTAUR CAN WAIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "minotaur_can_wait = True\n",
    "key_needed = False\n",
    "env_2 = mz.MinotaurMaze(maze,minotaur_can_wait,key_needed)\n",
    "#env_2.show()\n",
    "print('# states = ', env_2.n_states)\n",
    "print('# actions = ', env_2.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dynamic Programming \n",
    "\n",
    "Before solving the MDP problem, recall that the finite horizon objective function is \n",
    "$$\n",
    "    \\mathbb{E} \\Big[ \\sum_{t=0}^T r(s_t, a_t) \\Big],\n",
    "$$\n",
    "where $T$ is the horizon.\n",
    "Recall the Bellman equation \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V(s) = \\max_{a \\in \\mathcal{A}} \\Big\\lbrace r(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,a) V(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "The dynamic programming solution for the finite horizon MDP problem consists of solving the above backward recursion. The method `maze.dynamic_programming` achieves this. \n",
    "> **Note:** To find the optimal path, it is enough to set the time horizon $T = 10$. Indeed, looking at the maze one can see that the player needs at least 10 steps to attain the exit $B$, if her starting position is at $A$. In fact if you set the time horizon less than 10, you will see that you do not find the optimal path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy = mz.dynamic_programming(env_2,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'DynProg'\n",
    "path, victory_prob = env_2.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 1\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell probabilities\n",
    "cell_probs = env_2.minotaur_cell_probs(maze,minotaur_st,horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = horizon\n",
    "plt.imshow(cell_probs[t,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victory prob if the path is fixed\n",
    "fixed_path_vict_prob = env_2.fixed_path_vict_prob(maze,path,cell_probs)\n",
    "fixed_path_vict_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY THAT MAXIMIZES PROB FOR T = {1, ..., 30} AND PROBABILITY OF LEAVING THE MAZE FOR EACH T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_min = 1\n",
    "T_max = 30\n",
    "horizons = range(T_min,T_max+1)\n",
    "Vs, policys = [], []\n",
    "for horizon in horizons :\n",
    "    V, policy = mz.dynamic_programming(env_2,horizon)\n",
    "    Vs.append(V)\n",
    "    policys.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'DynProg'\n",
    "N = 5000\n",
    "# Simulate N paths for each time horizon\n",
    "mean_victory_probs = []\n",
    "for i in range(T_max) :\n",
    "    probs = []\n",
    "    for n in range(N) :\n",
    "        path, victory_prob = env_2.simulate(start, policys[i], method)\n",
    "        probs.append(victory_prob)\n",
    "    # Compute mean victory prob\n",
    "    mean_victory_probs.append(sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean victory prob for each horizon\n",
    "plt.plot(horizons, mean_victory_probs, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEOMETRICALLY DISTRIBUTED LIFE (TIME HORIZON) WITH MEAN 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "gamma = 29/30\n",
    "epsilon = 0.0001\n",
    "# Solve the MDP problem with value iteration\n",
    "V, policy = mz.value_iteration(env_1,gamma,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'ValIter'\n",
    "path, victory_prob = env_1.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 1\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBABILITY OF GETTING OUT ALIVE USING THE VALUE ITERATION POLICY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the minotaur cannot wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_T_max = 62\n",
    "E_T_min = 2\n",
    "E_Ts = range(E_T_min,E_T_max,2)\n",
    "Vs, policys = [], []\n",
    "gammas = []\n",
    "epsilon = 0.0001\n",
    "for E_T in E_Ts :\n",
    "    gamma = (E_T-1)/E_T\n",
    "    gammas.append(gamma)\n",
    "    V, policy = mz.value_iteration(env_1,gamma,epsilon)\n",
    "    Vs.append(V)\n",
    "    policys.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'ValIter'\n",
    "N = 5000\n",
    "# Simulate N paths for each gamma\n",
    "mean_victory_probs = []\n",
    "for i in range(len(gammas)) :\n",
    "    probs = []\n",
    "    for n in range(N) :\n",
    "        path, victory_prob = env_1.simulate(start, policys[i], method)\n",
    "        probs.append(victory_prob)\n",
    "    # Compute mean victory prob\n",
    "    mean_victory_probs.append(sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean victory prob for each gamma\n",
    "plt.plot(gammas, mean_victory_probs, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the minotaur can wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_T_max = 62\n",
    "E_T_min = 2\n",
    "E_Ts = range(E_T_min,E_T_max,2)\n",
    "Vs, policys = [], []\n",
    "gammas = []\n",
    "epsilon = 0.0001\n",
    "for E_T in E_Ts :\n",
    "    gamma = (E_T-1)/E_T\n",
    "    gammas.append(gamma)\n",
    "    V, policy = mz.value_iteration(env_2,gamma,epsilon)\n",
    "    Vs.append(V)\n",
    "    policys.append(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'ValIter'\n",
    "N = 5000\n",
    "# Simulate N paths for each gamma\n",
    "mean_victory_probs = []\n",
    "for i in range(len(gammas)) :\n",
    "    probs = []\n",
    "    for n in range(N) :\n",
    "        path, victory_prob = env_2.simulate(start, policys[i], method)\n",
    "        probs.append(victory_prob)\n",
    "    # Compute mean victory prob\n",
    "    mean_victory_probs.append(sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean victory prob for each gamma\n",
    "plt.plot(gammas, mean_victory_probs, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF WE NEED THE KEY TO SCAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting positions\n",
    "thomas_st = (0,0)\n",
    "minotaur_st = (6,5)\n",
    "\n",
    "# we need to pick the key (we dont have it from the beginning)\n",
    "key_st = (0,)\n",
    "\n",
    "# starting state\n",
    "start = (thomas_st + minotaur_st + key_st)\n",
    "\n",
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 3],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "# with the convention \n",
    "# 0 = empty cell\n",
    "# 1 = obstacle\n",
    "# 2 = exit of the Maze\n",
    "# 3 = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "minotaur_can_wait = False\n",
    "key_needed = True\n",
    "env_3 = mz.MinotaurMaze(maze,minotaur_can_wait,key_needed)\n",
    "#env_1.show()\n",
    "print('# states = ', env_3.n_states)\n",
    "print('# actions = ', env_3.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "gamma = 59/60\n",
    "epsilon = 0.0001\n",
    "# Solve the MDP problem with value iteration\n",
    "V, policy = mz.value_iteration(env_3,gamma,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'ValIter'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing learning rate and modifying epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.5\n",
    "Q, policy, init_Vs = mz.qLearning(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'Q-Learning'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon = 0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.2\n",
    "Q, policy, init_Vs = mz.qLearning(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'Q-Learning'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing $\\epsilon$ and modifying learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr_alpha = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.35\n",
    "Q, policy, init_Vs = mz.qLearning(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000, \n",
    "    max_iters=200, \n",
    "    decay_delta=0.,\n",
    "    lr_alpha = 0.55\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'Q-Learning'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr_alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.35\n",
    "Q, policy, init_Vs = mz.qLearning(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000, \n",
    "    max_iters=200, \n",
    "    decay_delta=0.,\n",
    "    lr_alpha = 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'Q-Learning'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing learning rate and modifying epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon = 0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.2\n",
    "Q, policy, init_Vs = mz.sarsa(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'SARSA'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "#mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.1\n",
    "Q, policy, init_Vs = mz.sarsa(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the best path (reacting to random minotaur actions)\n",
    "method = 'SARSA'\n",
    "path, victory_prob = env_3.simulate(start, policy, method)\n",
    "# Animate path \n",
    "show_arrows = True\n",
    "fps = 10\n",
    "#mz.animate_solution(maze, path, show_arrows, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decreasing $\\epsilon$ by $\\frac{1}{e^\\delta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 1\n",
    "Q, policy, init_Vs = mz.sarsa(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.75 # alpha lower than delta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 1\n",
    "Q, policy, init_Vs = mz.sarsa(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.55 # alpha greater than delta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(init_Vs)), init_Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prob. of winning with Q-LEARNING and SARSA policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.2\n",
    "Q, q_policy, q_init_Vs = mz.qLearning(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(q_init_Vs)), q_init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several paths and compute victory prob\n",
    "method = 'Q-Learning'\n",
    "N = 5000\n",
    "paths_dict = {}\n",
    "probs_dict = {}\n",
    "probs = []\n",
    "for n in range(N) :\n",
    "    path, victory_prob = env_3.simulate(start, q_policy, method)\n",
    "    paths_dict[n] = path\n",
    "    probs_dict[n] = victory_prob\n",
    "    probs.append(victory_prob)\n",
    "\n",
    "probs.sort()\n",
    "print('# SIMULATIONS = ', N)\n",
    "print('Mean Victory Prob = ', sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is Geo(1-gamma)\n",
    "gamma = 59/60\n",
    "# Initial exploration prob.\n",
    "epsilon = 0.2\n",
    "Q, s_policy, s_init_Vs = mz.sarsa(env_3, start, gamma, epsilon, \n",
    "    n_episodes=50000,\n",
    "    max_iters=200,\n",
    "    decay_delta=0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tot reward for each episode\n",
    "plt.plot(range(len(s_init_Vs)), s_init_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate several paths and compute victory prob\n",
    "method = 'SARSA'\n",
    "N = 5000\n",
    "paths_dict = {}\n",
    "probs_dict = {}\n",
    "probs = []\n",
    "for n in range(N) :\n",
    "    path, victory_prob = env_3.simulate(start, s_policy, method)\n",
    "    paths_dict[n] = path\n",
    "    probs_dict[n] = victory_prob\n",
    "    probs.append(victory_prob)\n",
    "\n",
    "probs.sort()\n",
    "print('# SIMULATIONS = ', N)\n",
    "print('Mean Victory Prob = ', sum(probs)/len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
